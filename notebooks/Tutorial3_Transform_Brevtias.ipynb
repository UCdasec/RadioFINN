{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio Modulation with FINN - Notebook #3 of 5\n",
    "\n",
    "***Input:*** Brevitas QONNX model \n",
    "\n",
    "***Output:*** FINN compatible \"FINN-ONNX\" model\n",
    "\n",
    "### Overview \n",
    "This notebooks walks you through performing **transformations** to conver the ONNX model into a FINN-ONNX model where **all the layers are compatible with FINN** and can be **converted to HLS/TRL in the next notebook**.\n",
    "1. Visualization of the model with Netron!\n",
    "2. The first **transformation:** **ConvertQONNXtoFINN**\n",
    "3. The second set of transformations: **Tidy transformations**\n",
    "4. The **removal of an unsupported node**, and corresponding adjustments \n",
    "5. Add a top-K node to the model (essentially telling the model to output the class with the highest probability)\n",
    "6. Setting of **datatypes** and saving the finn-onnx model\n",
    "   \n",
    "\n",
    "### FINN Pipeline Map\n",
    "Throughout these notebooks, you will begin to understand the FINN pipeline! In order the pipeline is:\n",
    "1. Dataset and Vanilla model\n",
    "2. Brevitas Model\n",
    "3. **Transforming the Brevitas Model to finn.onnx** (you are here)\n",
    "4. Transforming tidy.onnx to bitstream\n",
    "5. Loading the bitstream on the FPGA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Loading the Brevtias QONNX model from last notebook\n",
    "\n",
    "Reminder: \n",
    "1. **O**NNX is a general machine learning library with lots of tools, including an Intermediate Representation (IR) toolkit for defining and interacting with Neural Networks. ONNX also provides a standardize saving format, so we can save and load \".onnx\" files.\n",
    "2. **QO**NNX on the other hand uses ONNX's IR for neural networks and _extends_ some capabilites of ONNX. You have seen that Brevitas can be used to train a neural network... well alot of the magic behind Brevitas requires the use of *QO*NNX and it's ability to use datatypes that are _less than 8 bits_.\n",
    "3. **FINN** then takes models that are represented with *QO*NNX datastructures as inputs and _compiles_ then. (More on this in future notebooks)\n",
    "\n",
    "That is a high level overview of where QONNX sits in the pipeline. Using there wrapper around ONNX, we can now prepare the model for the FINN compiler. The first step is to load our Brevitas Model file, and wrap it with a QONNX \"ModelWrapper\", which essentially exposes QONNX capabilites on a loaded *.onnx file! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tools/Xilinx/Vivado/2024.1\n",
      "/tools/Xilinx/Vitis_HLS/2024.1\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.util.visualization import showInNetron\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "import onnx\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.general import (\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    ")\n",
    "\n",
    "#Path to the qonnx model exported by brevitas\n",
    "brevitas_model_pth='27ml_rf/models/radio_27ml_brevitas.onnx'\n",
    "model=ModelWrapper(brevitas_model_pth)\n",
    "\n",
    "# Here we print 2 required environment variables. These should point to your /../Vivado/ path and your /../Vivado/Vitis_HLS path \n",
    "# which provide required libraries for FINN.\n",
    "!echo $VIVADO_PATH\n",
    "!echo $HLS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Visualizing the Brevitas ONNX model using `showInNetron`\n",
    "\n",
    "***Netron*** is a webapp that displays *a graph of a Nueral Network*! \n",
    "\n",
    "Using this we can obseve the nodes, their properties, and the connections in the Neural Network. This is particularly helpful in this pipeline because some nodes are _not supported by finn_. To fix this, we observe the nodes in the Netron Output and conduct a series of transformation to preserve the model's overall architecure, while making it compatible with *finn*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '27ml_rf/models/radio_27ml_brevitas.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7e9c88bbac20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_machine_ip='localhost'\n",
    "assert host_machine_ip!='your host machine IP', print('host_machine_ip not set')\n",
    "showInNetron(brevitas_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Converting the model to FINN-ONNX model\n",
    "\n",
    "We call a **ONNX** model **FINN-ONNX** when it _only has layers and an architecture supported by finn_.\n",
    "\n",
    "In the below cell we start the process of this conversion. Specifically, the first step is to **initialize** the model with it's first transform \"ConvertQONNXtoFINN\". This takes a model that is currently wrapped as a QONNX model, and creates an object that we can now progressively transform until it is ready for FINN (as well provides some automatic transformations under the hood!)\n",
    "\n",
    "For example, \n",
    "Onnx has something called a Gemm transformation. It stands for generanal matrix multiplication. The Doc is here: https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Gemm-9\n",
    "\n",
    "FINN does not support this. And because the QONNX network still has them, we must convert it to a standard MatMul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/ghPackages/RadioFINN/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "finn_model_pth ='27ml_rf/models/radio_27ml_inital_finn.onnx'\n",
    "model.save(finn_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing FINN-QONNX model. \n",
    "Feel free to notice the changes below, **the graph looks very different**, and this was all handled by the first transformation.\n",
    "\n",
    "FINN handles alot of the more backend of the transformation. Therefore all that is generally required by us it was manual transformations we must make and why. In the next cell we take a look out the first manual transformation. \n",
    "\n",
    "***Notice*** the \"MultiThreshold\" node added by the FINN backend. This node uses threshold determined by the backend of FINN to \"bin\" input values to values within range of the quantization defined by our quant nodes!\n",
    "\n",
    "as ADC - analog to digital converter, as it takes the input and uses different thresholds to quantize our. Specifically, the input is \"binned\" to a \"quantized value\" based thresholds determined by the backend of FINN using our Quantizer Layers! \n",
    "\n",
    "The connection between ADC and quantization is interesting! Both are \"stepping down\" a signal of more samples (in the adc's case continuous) to a discrete signal in a smaller range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_inital_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7e9c88bb9db0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tidy Transformations\n",
    "This transform is named \"Tidy\". Specifically, we provide a list of 4 transformations that \"tidy\" the model graph up. This is _required_ for the pipeline (and I wish FINN automatically did it... maybe we can open a PR for this?). \n",
    "\n",
    "In the above netron cell, click on the first \"Add\" node you see and observe the bold header \"INPUT\". Here you'll see the names of the inputs to the \"Add\" node are something similar to pTtSzD. In otherwords, un-descriptive names. \n",
    "\n",
    "Well in the Tidy transformation we fix this ! Each node, and it's inputs recieves a more readable name. We also:\n",
    "1. Infer the shapes of each node\n",
    "2. Infer the datatype of each node and attach this\n",
    "   \n",
    "All of these are attached as _attributes_ to the node. These properties can be viewed by observing the model in netron below\n",
    "\n",
    "***Do note*** we do not choose to do this; it is _required_ for the FINN pipeline as _something_ in the backend of the FINN library requires these. \n",
    "\n",
    "***TODO:*** FINN in its most recent version _already_ has the node and inputs and outputs shapes, having to infer them in a tidy transformation may be depreciated. For now, this should be kept just in case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload our FINN model. \n",
    "finn_model = ModelWrapper(finn_model_pth)\n",
    "\n",
    "# Define our transforms\n",
    "transforms = [\n",
    "    InferShapes(),\n",
    "    InferDataTypes(),\n",
    "    GiveUniqueNodeNames(),\n",
    "    GiveReadableTensorNames(),\n",
    "]\n",
    "\n",
    "for transform in transforms:\n",
    "    finn_model = finn_model.transform(transform)\n",
    "\n",
    "finn_model.cleanup()\n",
    "\n",
    "pre_net_surgery_pth='27ml_rf/models/radio_27ml_pre_nw_surgery.onnx'\n",
    "finn_model.save(pre_net_surgery_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_pre_nw_surgery.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7e9c88bba890>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(pre_net_surgery_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Network Surgery\n",
    "In the graph above, notice the first \"MultiThreshold\". It was actually the backend of FINN that created this node, but we actually must remove it! \n",
    "\n",
    "Specifically, FINN does not support a model where the first node it \"input\" -> \"MultiThreshold\". To fix this we manually remove both the \"input\" and the \"MultiThreshold\".\n",
    "\n",
    "The change in the network, reflected by the diagram in Netron will look like this:\n",
    "\n",
    "**Original:** `input`--> `MultiThreshold`--> `Add`--> `Conv` -->...\n",
    "\n",
    "**Post-Surgery:** `Add`--> `Conv` --> ...\n",
    "\n",
    "However, during training, the input of the `Add` node is the output of `MultiThreshold`. Therefore, instead of passing through the raw data to our new model, we will manually perform what `MultiThreshold` does to our raw data (quantizing data) outside the model, then pass it through the new model as input.\n",
    "\n",
    "Further information can be found in this finn's discussion: https://github.com/Xilinx/finn/discussions/420\n",
    "\n",
    "**Notice**: This network surgery only works for this specific model architecture (VGG-10). If a different architecture is used, the network surgery might be different or not required. Ultimately, the goal is to get a streamlined model before generating hardware layers and this provides a good example.\n",
    "\n",
    "#### Preform the surgery. \n",
    "In the below cell we conduct the network surgery by first finding the `Conv`, `Add`, and the `original input` nodes. Then second we remove (1) the original input node and (2) assign the `Add` node to be the new input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the first 'Conv' node and store it in 'new_input_node'\n",
    "first_conv_node = finn_model.get_nodes_by_op_type(\"Conv\")[0]   \n",
    "#Find the input of that 'Conv' node (in this case it is the 'Add' node)\n",
    "new_input_tensor = finn_model.get_tensor_valueinfo(first_conv_node.input[0]) \n",
    "\n",
    "#Find the original input node of the model.\n",
    "old_input_tensor = finn_model.graph.input[0] \n",
    "\n",
    "#Remove the old input node, and replace it with the new input tensor ('Add' node)\n",
    "finn_model.graph.input.remove(old_input_tensor) \n",
    "finn_model.graph.input.append(new_input_tensor)\n",
    "\n",
    "#Find the index of the new input node, and remove everything from index 0 to that index\n",
    "#In this case, we will be removing index 0 and index 1, which are the 'inp' and 'MultiThreshold' nodes\n",
    "#So now, the 'Add' node become the model input with index 0, and the 'Conv' node has index 1, and so on...\n",
    "new_input_index = finn_model.get_node_index(first_conv_node)\n",
    "del finn_model.graph.node[0:new_input_index]\n",
    "\n",
    "pre_net_surgery_pth='27ml_rf/models/radio_27ml_pre_nw_surgery.onnx'\n",
    "finn_model.save(pre_net_surgery_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_pre_nw_surgery.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7cec7810c760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(pre_net_surgery_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Insert TopK node.\n",
    "\n",
    "The output of our model is 27 values, each corresponding to the 27 classes, where the greatest value is used as the predicted class.\n",
    "\n",
    "We must explicitly \"tell\" the model to return just a single value: The predicted class. To do this we must add a TopK node, where the TopK values out of the 27 are returned as the predictions. Because we only require the predicted class, we set k=1, and return just the most probable class. \n",
    "\n",
    "**Also note** There was an issue with FINN rejecting models due to redunant information in the \"finn model graph\". Specifically, the input and output nodes were causing an issue, and thus in the below cell we show how that information can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "# insert topK node, with k=1, meaning we pick the 1 classification with highest prediction value\n",
    "finn_model = finn_model.transform(InsertTopK(k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling compatibility and set the input datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "# manually set input datatype (not done by brevitas yet)\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finn_model.set_tensor_datatype(finnonnx_in_tensor_name, DataType[\"INT8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: Add_0_out0\n",
      "Input tensor shape: [1, 2, 1024]\n",
      "Input tensor datatype: INT8\n",
      "Modified FINN-ready model saved to 27ml_rf/models/radio_27ml_finn.onnx\n"
     ]
    }
   ],
   "source": [
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Input tensor datatype: %s\" % str(finn_model.get_tensor_datatype(finnonnx_in_tensor_name)))\n",
    "\n",
    "# save modified model that is now ready for the FINN compiler\n",
    "tidy_model_pth='27ml_rf/models/radio_27ml_finn.onnx'\n",
    "finn_model.save(tidy_model_pth)\n",
    "print(\"Modified FINN-ready model saved to %s\" % tidy_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the new model\n",
    "Notice how the new model now has the `Add` node is the input. Everything from the old input node to the `Add` node is now removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7cec7810cfa0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualise the new model\n",
    "showInNetron(tidy_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
