{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio Modulation with FINN - Notebook #3 of 5\n",
    "This notebook walks you through performing a network surgery on the ONNX model. In this notebook, we introduce:\n",
    "1. Usage of ModelWrapper() and showInNetron()\n",
    "1. Getting a node in the model by their name \n",
    "2. Getting a node by getting the input of its succeeding node\n",
    "3. Removing a node from the model\n",
    "4. Changing the input and its datatype of the model\n",
    "5. Add a top-K node to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting QONNX format to FINN-QONNX format\n",
    "\n",
    "QONNX format is from the Brevitas library. To run transformation through FINN, we need to convert the model to a FINN-QONNX format.\n",
    "\n",
    "Whenever we want to instantiate an onnx model, we can use `ModelWrapper(filepath)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the brevitas QONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phu/Vivado/Vitis_HLS/2024.1\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.util.visualization import showInNetron\n",
    "#Path to the qonnx model exported by brevitas\n",
    "brevitas_model_pth='27ml_rf/models/radio_27ml_export.onnx'\n",
    "\n",
    "model=ModelWrapper(brevitas_model_pth)\n",
    "\n",
    "!echo $VIVADO_PATH\n",
    "!echo $HLS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Brevitas ONNX model using `showInNetron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '27ml_rf/models/radio_27ml_export.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7283c45c5690>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_machine_ip='localhost'\n",
    "assert host_machine_ip!='your host machine IP', print('host_machine_ip not set')\n",
    "showInNetron(brevitas_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the model to FINN-ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/ghPackages/RadioFINN/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "finn_model_pth='27ml_rf/models/radio_27ml_finn.onnx'\n",
    "model.save(finn_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing FINN-QONNX model. \n",
    "Notice how it is much easier to read compared to Brevitas ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7283c45c68c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Surgery\n",
    "From the graph above, you can see that the `input` goes through `MultiThreshold`, then `Add`, then `Conv` nodes. The problem we are facing is that FINN is unable to streamline the first `MultiThreshold` node. This is why we need to remove them manually.\n",
    "\n",
    "Originally\n",
    "\n",
    "`input`--> `MultiThreshold`--> `Add`--> `Conv` -->...\n",
    "\n",
    "After network surgery:\n",
    "\n",
    "`Add`--> `Conv` --> ...\n",
    "\n",
    "However, during training, the input of `Add` node expects the output of `MultiThreshold`. Therefore, instead of passing through the raw data to our new model, we will manually perform what `MultiThreshold` does to our raw data (quantizing data) outside the model, then pass it through the new model as input.\n",
    "\n",
    "\n",
    "Further information can be found in this finn's discussion: https://github.com/Xilinx/finn/discussions/420\n",
    "\n",
    "**Notice**: This network surgery only works for this specific model architechture (VGG-10). If a different architechture is used, the network surgery might be different or even not required. Ultimately, the goal is allow the model to be fully streamlined before generating hardware layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidying up the model\n",
    "This step does not change parameters in the model, it is purely for reformatting the labels and renaming layers name for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_pre_nw_surgery.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7283beb783a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.general import (\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    ")\n",
    "import onnx\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "# tidy up\n",
    "finn_model = ModelWrapper(finn_model_pth)\n",
    "\n",
    "finn_model = finn_model.transform(InferShapes())\n",
    "finn_model = finn_model.transform(InferDataTypes())\n",
    "finn_model = finn_model.transform(GiveUniqueNodeNames())\n",
    "finn_model = finn_model.transform(GiveReadableTensorNames())\n",
    "finn_model.cleanup()\n",
    "\n",
    "pre_net_surgery_pth='27ml_rf/models/radio_27ml_pre_nw_surgery.onnx'\n",
    "finn_model.save(pre_net_surgery_pth)\n",
    "\n",
    "showInNetron(pre_net_surgery_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the `Conv`, `Add`, and the `original input` nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_model=ModelWrapper(pre_net_surgery_pth)\n",
    "\n",
    "#Find the first 'Conv' node and store it in 'new_input_node'\n",
    "new_input_node = finn_model.get_nodes_by_op_type(\"Conv\")[0]   \n",
    "#Find the input of that 'Conv' node (in this case it is the 'Add' node)\n",
    "new_input_tensor = finn_model.get_tensor_valueinfo(new_input_node.input[0]) \n",
    "\n",
    "#Find the original input node of the model.\n",
    "old_input_tensor = finn_model.graph.input[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the original input node, assigning the `Add` node to be the new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the old input node, and replace it with the new input tensor ('Add' node)\n",
    "finn_model.graph.input.remove(old_input_tensor) \n",
    "finn_model.graph.input.append(new_input_tensor)\n",
    "\n",
    "#Find the index of the new input node, and remove everything from index 0 to that index\n",
    "#In this case, we will be removing index 0 and index 1, which are the 'inp' and 'MultiThreshold' nodes\n",
    "#So now, the 'Add' node become the model input with index 0, and the 'Conv' node has index 1, and so on...\n",
    "new_input_index = finn_model.get_node_index(new_input_node)\n",
    "del finn_model.graph.node[0:new_input_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any `softmax` node and insert a `topK` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing: remove final softmax node from training\n",
    "# Removing any softmax node if there is any. This is because finn will use topK instead of softmax\n",
    "# We have already removed the softmax node when building the model, so this can be ignored\n",
    "softmax_node = finn_model.graph.node[-1]\n",
    "softmax_in_tensor = finn_model.get_tensor_valueinfo(softmax_node.input[0])\n",
    "softmax_out_tensor = finn_model.get_tensor_valueinfo(softmax_node.output[0])\n",
    "finn_model.graph.output.remove(softmax_out_tensor)\n",
    "finn_model.graph.output.append(softmax_in_tensor)\n",
    "finn_model.graph.node.remove(softmax_node)\n",
    "\n",
    "# insert topK node in place of the final softmax node\n",
    "# topK plays similar role to softmax\n",
    "# k=1 means it pick only 1 classification with highest predictions value\n",
    "finn_model = finn_model.transform(InsertTopK(k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling compatibility and set the input datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "# manually set input datatype (not done by brevitas yet)\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finn_model.set_tensor_datatype(finnonnx_in_tensor_name, DataType[\"INT8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Input tensor datatype: %s\" % str(finn_model.get_tensor_datatype(finnonnx_in_tensor_name)))\n",
    "\n",
    "# save modified model that is now ready for the FINN compiler\n",
    "tidy_model_pth='27ml_rf/models/radio_27ml_tidy.onnx'\n",
    "finn_model.save(tidy_model_pth)\n",
    "print(\"Modified FINN-ready model saved to %s\" % tidy_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the new model\n",
    "Notice how the new model now has the `Add` node is the input. Everything from the old input node to the `Add` node is now removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the new model\n",
    "showInNetron(tidy_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
