{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio Modulation with FINN - Notebook #3 of 5\n",
    "This notebook walks you through performing a network surgery on the ONNX model. In this notebook, we introduce:\n",
    "1. Usage of ModelWrapper() and showInNetron()\n",
    "1. Getting a node in the model by their name \n",
    "2. Getting a node by getting the input of its succeeding node\n",
    "3. Removing a node from the model\n",
    "4. Changing the input and its datatype of the model\n",
    "5. Add a top-K node to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting QONNX format to FINN-QONNX format\n",
    "\n",
    "QONNX format is from the Brevitas library. To run transformation through FINN, we need to convert the model to a FINN-QONNX format.\n",
    "\n",
    "Whenever we want to instantiate an onnx model, we can use `ModelWrapper(filepath)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the brevitas QONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phu/Vivado/Vivado/2024.1\n",
      "/home/phu/Vivado/Vitis_HLS/2024.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phu/Vivado/Vitis_HLS/2024.1\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "#Path to the qonnx model exported by brevitas\n",
    "brevitas_model_pth='27ml_rf/models/radio_27ml_export.onnx'\n",
    "\n",
    "\n",
    "model=ModelWrapper(brevitas_model_pth)\n",
    "\n",
    "#Checking if the docker generated correct environment variable\n",
    "!echo $VIVADO_PATH\n",
    "!echo $HLS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Brevitas ONNX model using `showInNetron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '27ml_rf/models/radio_27ml_export.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27ac216cb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "def check_ip_add(addr):\n",
    "    try:\n",
    "        socket.inet_aton(host_machine_ip)\n",
    "        pass\n",
    "        # legal\n",
    "    except socket.error:\n",
    "        assert False, print('host_machine_ip invalid')\n",
    "        # Not legal\n",
    "        \n",
    "host_machine_ip='change_this_to_host_ip'\n",
    "\n",
    "check_ip_add(host_machine_ip)\n",
    "showInNetron(brevitas_model_pth,localhost_url=host_machine_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the model to FINN-ONNX model\n",
    "\n",
    "Onnx has something called a Gemm transformation. It stands for generanal matrix multiplication. The Doc is here: https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Gemm-9\n",
    "\n",
    "FINN does not support this. And because the QONNX network still has them, we must convert it to a standard MatMul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "finn_model_pth='27ml_rf/models/radio_27ml_finn.onnx'\n",
    "model.save(finn_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing FINN-QONNX model. \n",
    "Notice how it is much easier to read compared to Brevitas ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8082\n",
      "Serving '27ml_rf/models/radio_27ml_finn.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27ac216d40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_model_pth,localhost_url=host_machine_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Surgery\n",
    "From the graph above, you can see that the `input` goes through `MultiThreshold`, then `Add`, then `Conv` nodes. The problem we are facing is that FINN is unable to streamline the first `MultiThreshold` node. This is why we need to remove them manually.\n",
    "\n",
    "Originally\n",
    "\n",
    "`input`--> `MultiThreshold`--> `Add`--> `Conv` -->...\n",
    "\n",
    "After network surgery:\n",
    "\n",
    "`Add`--> `Conv` --> ...\n",
    "\n",
    "However, during training, the input of `Add` node expects the output of `MultiThreshold`. Therefore, instead of passing through the raw data to our new model, we will manually perform what `MultiThreshold` does to our raw data (quantizing data) outside the model, then pass it through the new model as input.\n",
    "\n",
    "\n",
    "Further information can be found in this finn's discussion: https://github.com/Xilinx/finn/discussions/420\n",
    "\n",
    "**Notice**: This network surgery only works for this specific model architechture (VGG-10). If a different architechture is used, the network surgery might be different or even not required. Ultimately, the goal is allow the model to be fully streamlined before generating hardware layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidying up the model\n",
    "This step does not change parameters in the model, it is purely for reformatting the labels and renaming layers name for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.general import (\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    ")\n",
    "import onnx\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "# tidy up - Changing node names For example A\n",
    "finn_model = ModelWrapper(finn_model_pth)\n",
    "\n",
    "finn_model = finn_model.transform(InferShapes())    \n",
    "finn_model = finn_model.transform(InferDataTypes())\n",
    "finn_model = finn_model.transform(GiveUniqueNodeNames())\n",
    "finn_model = finn_model.transform(GiveReadableTensorNames())\n",
    "finn_model.cleanup()\n",
    "\n",
    "pre_net_surgery_pth='27ml_rf/models/radio_27ml_pre_nw_surgery.onnx'\n",
    "finn_model.save(pre_net_surgery_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8082\n",
      "Serving '27ml_rf/models/radio_27ml_pre_nw_surgery.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27ac215180>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(pre_net_surgery_pth,localhost_url=host_machine_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the `Conv`, `Add`, and the `original input` nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finn_model=ModelWrapper(pre_net_surgery_pth)\n",
    "\n",
    "#Find the first 'Conv' node and store it in 'new_input_node'\n",
    "new_input_node = finn_model.get_nodes_by_op_type(\"Conv\")[0]   \n",
    "#Find the input of that 'Conv' node (in this case it is the 'Add' node)\n",
    "new_input_tensor = finn_model.get_tensor_valueinfo(new_input_node.input[0]) \n",
    "\n",
    "#Find the original input node of the model.\n",
    "old_input_tensor = finn_model.graph.input[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the original input node, assigning the `Add` node to be the new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the old input node, and replace it with the new input tensor ('Add' node)\n",
    "finn_model.graph.input.remove(old_input_tensor) \n",
    "finn_model.graph.input.append(new_input_tensor)\n",
    "\n",
    "#Find the index of the new input node, and remove everything from index 0 to that index\n",
    "#In this case, we will be removing index 0 and index 1, which are the 'inp' and 'MultiThreshold' nodes\n",
    "#So now, the 'Add' node become the model input with index 0, and the 'Conv' node has index 1, and so on...\n",
    "new_input_index = finn_model.get_node_index(new_input_node)\n",
    "del finn_model.graph.node[0:new_input_index]\n",
    "pre_net_surgery_pth='27ml_rf/models/radio_27ml_pre_nw_surgery.onnx'\n",
    "finn_model.save(pre_net_surgery_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should see the graph of the new model has the `Add` node being the new model input, followed directly by the `Conv`. The previous `inp` and `MultiThreshold` nodes are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8082\n",
      "Serving '27ml_rf/models/radio_27ml_pre_nw_surgery.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27ae46d150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(pre_net_surgery_pth,localhost_url=host_machine_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any `softmax` node and insert a `topK` node\n",
    "\n",
    "- If theres a softmax, FINN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing: remove final softmax node from training\n",
    "# Removing any softmax node if there is any. This is because finn will use topK instead of softmax\n",
    "# We have already removed the softmax node when building the model, so this can be ignored\n",
    "#softmax_node = finn_model.graph.node[-1]\n",
    "#softmax_in_tensor = finn_model.get_tensor_valueinfo(softmax_node.input[0])\n",
    "#softmax_out_tensor = finn_model.get_tensor_valueinfo(softmax_node.output[0])\n",
    "#finn_model.graph.output.remove(softmax_out_tensor)\n",
    "#finn_model.graph.output.append(softmax_in_tensor)\n",
    "#finn_model.graph.node.remove(softmax_node)\n",
    "\n",
    "\n",
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "\n",
    "# insert topK node in place of the final softmax node\n",
    "# topK plays similar role to softmax\n",
    "# k=1 means it pick only 1 classification with highest predictions value\n",
    "finn_model = finn_model.transform(InsertTopK(k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling compatibility and set the input datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "# manually set input datatype (not done by brevitas yet)\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finn_model.set_tensor_datatype(finnonnx_in_tensor_name, DataType[\"INT8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: Add_0_out0\n",
      "Input tensor shape: [1, 2, 1024]\n",
      "Input tensor datatype: INT8\n",
      "Modified FINN-ready model saved to 27ml_rf/models/radio_27ml_tidy.onnx\n"
     ]
    }
   ],
   "source": [
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Input tensor datatype: %s\" % str(finn_model.get_tensor_datatype(finnonnx_in_tensor_name)))\n",
    "\n",
    "# save modified model that is now ready for the FINN compiler\n",
    "tidy_model_pth='27ml_rf/models/radio_27ml_tidy.onnx'\n",
    "finn_model.save(tidy_model_pth)\n",
    "print(\"Modified FINN-ready model saved to %s\" % tidy_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the new model\n",
    "Notice how the new model now has the `Add` node is the input. Everything from the old input node to the `Add` node is now removed.\n",
    "\n",
    "An additional topK node is also added at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8082\n",
      "Serving '27ml_rf/models/radio_27ml_tidy.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f27ae46e410>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualise the new model\n",
    "showInNetron(tidy_model_pth,localhost_url=host_machine_ip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
