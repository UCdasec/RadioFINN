{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bc849a-ba94-4325-91f6-a782778efe28",
   "metadata": {},
   "source": [
    "# Radio Modulation Classification with FINN - Notebook #3 of 5\n",
    "\n",
    "### Overview \n",
    "In this Notebook we will transform the Brevitas model from last notebook to a tidy.onnx file! \n",
    "\n",
    "\n",
    "### FINN Pipeline Map\n",
    "Throughout these notebooks, you will begin to understand the FINN pipeline! In order the pipeline is:\n",
    "1. Dataset and Vanilla model\n",
    "2. Brevitas Model\n",
    "3. **Transforming the Brevitas Model to tidy.onnx** (you are here)\n",
    "4. Transforming tidy.onnx to bitstream\n",
    "5. Loading the bitstream on the FPGA!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84b6319-5288-4cb0-a61e-a9882cf7f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters will be saved in 27ml_rf/27ml_rf_quantized.pth\n",
      "model is saved in 27ml_rf/models/radio_27ml_export.onnx\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# A qnn is a Brevitas version of pytorch's nn. nn stands for neural network.\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "# Adjustable hyperparameters\n",
    "input_bits = 8\n",
    "a_bits = 8  # a_bits is the bit width for ReLu\n",
    "w_bits = 8 # w_bits is the bit width for all the weights\n",
    "filters_conv = 64\n",
    "filters_dense = 128\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    #Using 8 bits\n",
    "    bit_width = input_bits\n",
    "    #Converting from int8 [-127,128] to finn-float32 [-127.0,128.0]\n",
    "    #Since our dataset is already quantized before going through the model,\n",
    "    #   this would only convert the datatype, the values would be the same\n",
    "    #Refer to this notebook below, which is used for the 2018 dataset,\n",
    "    #to see the difference between this version and the 2018 version\n",
    "    #(https://github.com/Xilinx/brevitas-radioml-challenge-21/blob/main/sandbox/notebooks/training_and_evaluation.ipynb)\n",
    "    min_val = -127.0\n",
    "    max_val = 128.0\n",
    "\n",
    "    # \n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "model_class = nn.Sequential(\n",
    "    # Input quantization layer\n",
    "    qnn.QuantHardTanh(act_quant=InputQuantizer),\n",
    "\n",
    "    qnn.QuantConv1d(2, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits,bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    qnn.QuantLinear(filters_conv*8, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits, return_quant_tensor=True),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, 27, weight_bit_width=w_bits, bias=True, bias_quant=Int8Bias),\n",
    ")\n",
    "\n",
    "model_name='27ml_rf'\n",
    "model_file_name='27ml_rf_quantized.pth'\n",
    "\n",
    "Path(model_name).mkdir(exist_ok=True)\n",
    "chpt_path=model_name+'/'+model_file_name\n",
    "print(f'Model parameters will be saved in {chpt_path}')\n",
    "\n",
    "#Load the model back again, or you can change the file to load a different model\n",
    "#Redefining the model class because we only save the parameters, not the structure of the model\n",
    "load_path=chpt_path #Change this to a path of a different model\n",
    "model=model_class #Reinitialize the class of the model, so we can fill the parameters in\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "build_dir=\"27ml_rf/models\" #Directory to save model\n",
    "#Ensuring path exist, otherwise create an empty directory\n",
    "Path(build_dir).mkdir(exist_ok=True)\n",
    "export_path=f\"{build_dir}/radio_27ml_export.onnx\" #Full name of the path of the model with the tail _export.onnx\n",
    "export_qonnx(model.to('cuda'), torch.randn(1, 2, 1024).to('cuda'), export_path=export_path);\n",
    "\n",
    "print(f'model is saved in {export_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fecfa77e-0149-480b-8033-edbb75108d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/ghPackages/RadioFINN/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input quant thresholds\n",
      "[[-127.5 -126.5 -125.5 -124.5 -123.5 -122.5 -121.5 -120.5 -119.5 -118.5\n",
      "  -117.5 -116.5 -115.5 -114.5 -113.5 -112.5 -111.5 -110.5 -109.5 -108.5\n",
      "  -107.5 -106.5 -105.5 -104.5 -103.5 -102.5 -101.5 -100.5  -99.5  -98.5\n",
      "   -97.5  -96.5  -95.5  -94.5  -93.5  -92.5  -91.5  -90.5  -89.5  -88.5\n",
      "   -87.5  -86.5  -85.5  -84.5  -83.5  -82.5  -81.5  -80.5  -79.5  -78.5\n",
      "   -77.5  -76.5  -75.5  -74.5  -73.5  -72.5  -71.5  -70.5  -69.5  -68.5\n",
      "   -67.5  -66.5  -65.5  -64.5  -63.5  -62.5  -61.5  -60.5  -59.5  -58.5\n",
      "   -57.5  -56.5  -55.5  -54.5  -53.5  -52.5  -51.5  -50.5  -49.5  -48.5\n",
      "   -47.5  -46.5  -45.5  -44.5  -43.5  -42.5  -41.5  -40.5  -39.5  -38.5\n",
      "   -37.5  -36.5  -35.5  -34.5  -33.5  -32.5  -31.5  -30.5  -29.5  -28.5\n",
      "   -27.5  -26.5  -25.5  -24.5  -23.5  -22.5  -21.5  -20.5  -19.5  -18.5\n",
      "   -17.5  -16.5  -15.5  -14.5  -13.5  -12.5  -11.5  -10.5   -9.5   -8.5\n",
      "    -7.5   -6.5   -5.5   -4.5   -3.5   -2.5   -1.5   -0.5    0.5    1.5\n",
      "     2.5    3.5    4.5    5.5    6.5    7.5    8.5    9.5   10.5   11.5\n",
      "    12.5   13.5   14.5   15.5   16.5   17.5   18.5   19.5   20.5   21.5\n",
      "    22.5   23.5   24.5   25.5   26.5   27.5   28.5   29.5   30.5   31.5\n",
      "    32.5   33.5   34.5   35.5   36.5   37.5   38.5   39.5   40.5   41.5\n",
      "    42.5   43.5   44.5   45.5   46.5   47.5   48.5   49.5   50.5   51.5\n",
      "    52.5   53.5   54.5   55.5   56.5   57.5   58.5   59.5   60.5   61.5\n",
      "    62.5   63.5   64.5   65.5   66.5   67.5   68.5   69.5   70.5   71.5\n",
      "    72.5   73.5   74.5   75.5   76.5   77.5   78.5   79.5   80.5   81.5\n",
      "    82.5   83.5   84.5   85.5   86.5   87.5   88.5   89.5   90.5   91.5\n",
      "    92.5   93.5   94.5   95.5   96.5   97.5   98.5   99.5  100.5  101.5\n",
      "   102.5  103.5  104.5  105.5  106.5  107.5  108.5  109.5  110.5  111.5\n",
      "   112.5  113.5  114.5  115.5  116.5  117.5  118.5  119.5  120.5  121.5\n",
      "   122.5  123.5  124.5  125.5  126.5]\n",
      " [-127.5 -126.5 -125.5 -124.5 -123.5 -122.5 -121.5 -120.5 -119.5 -118.5\n",
      "  -117.5 -116.5 -115.5 -114.5 -113.5 -112.5 -111.5 -110.5 -109.5 -108.5\n",
      "  -107.5 -106.5 -105.5 -104.5 -103.5 -102.5 -101.5 -100.5  -99.5  -98.5\n",
      "   -97.5  -96.5  -95.5  -94.5  -93.5  -92.5  -91.5  -90.5  -89.5  -88.5\n",
      "   -87.5  -86.5  -85.5  -84.5  -83.5  -82.5  -81.5  -80.5  -79.5  -78.5\n",
      "   -77.5  -76.5  -75.5  -74.5  -73.5  -72.5  -71.5  -70.5  -69.5  -68.5\n",
      "   -67.5  -66.5  -65.5  -64.5  -63.5  -62.5  -61.5  -60.5  -59.5  -58.5\n",
      "   -57.5  -56.5  -55.5  -54.5  -53.5  -52.5  -51.5  -50.5  -49.5  -48.5\n",
      "   -47.5  -46.5  -45.5  -44.5  -43.5  -42.5  -41.5  -40.5  -39.5  -38.5\n",
      "   -37.5  -36.5  -35.5  -34.5  -33.5  -32.5  -31.5  -30.5  -29.5  -28.5\n",
      "   -27.5  -26.5  -25.5  -24.5  -23.5  -22.5  -21.5  -20.5  -19.5  -18.5\n",
      "   -17.5  -16.5  -15.5  -14.5  -13.5  -12.5  -11.5  -10.5   -9.5   -8.5\n",
      "    -7.5   -6.5   -5.5   -4.5   -3.5   -2.5   -1.5   -0.5    0.5    1.5\n",
      "     2.5    3.5    4.5    5.5    6.5    7.5    8.5    9.5   10.5   11.5\n",
      "    12.5   13.5   14.5   15.5   16.5   17.5   18.5   19.5   20.5   21.5\n",
      "    22.5   23.5   24.5   25.5   26.5   27.5   28.5   29.5   30.5   31.5\n",
      "    32.5   33.5   34.5   35.5   36.5   37.5   38.5   39.5   40.5   41.5\n",
      "    42.5   43.5   44.5   45.5   46.5   47.5   48.5   49.5   50.5   51.5\n",
      "    52.5   53.5   54.5   55.5   56.5   57.5   58.5   59.5   60.5   61.5\n",
      "    62.5   63.5   64.5   65.5   66.5   67.5   68.5   69.5   70.5   71.5\n",
      "    72.5   73.5   74.5   75.5   76.5   77.5   78.5   79.5   80.5   81.5\n",
      "    82.5   83.5   84.5   85.5   86.5   87.5   88.5   89.5   90.5   91.5\n",
      "    92.5   93.5   94.5   95.5   96.5   97.5   98.5   99.5  100.5  101.5\n",
      "   102.5  103.5  104.5  105.5  106.5  107.5  108.5  109.5  110.5  111.5\n",
      "   112.5  113.5  114.5  115.5  116.5  117.5  118.5  119.5  120.5  121.5\n",
      "   122.5  123.5  124.5  125.5  126.5]]\n",
      "Input tensor name: Add_0_out0\n",
      "Input tensor shape: [1, 2, 1024]\n",
      "Input tensor datatype: INT8\n",
      "Modified FINN-ready model saved to 27ml_rf/models/radio_27ml_tidy.onnx\n"
     ]
    }
   ],
   "source": [
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.general import (\n",
    "    ConvertSubToAdd,\n",
    "    ConvertDivToMul,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    SortGraph,\n",
    "    RemoveUnusedTensors,\n",
    "    GiveUniqueParameterTensors,\n",
    "    RemoveStaticGraphInputs,\n",
    "    ApplyConfig,\n",
    ")\n",
    "import onnx\n",
    "from onnx import TensorProto, helper\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import torch\n",
    "import onnx\n",
    "from brevitas.export import export_qonnx,export_brevitas_onnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "import os \n",
    "import onnx\n",
    "\n",
    "\n",
    "\n",
    "#Main goal is to skip the first multithreshold node.\n",
    "#The first multithreshold node is the quantizing data node.\n",
    "#Because on FPGA we will preprocess the data before forwarding through model, \n",
    "#we will remove the first multithreshold node.\n",
    "model = ModelWrapper(export_path)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(f\"{build_dir}/radio_27ml_finn.onnx\")\n",
    "\n",
    "# tidy up\n",
    "finn_model = ModelWrapper(f\"{build_dir}/radio_27ml_finn.onnx\")\n",
    "\n",
    "finn_model = finn_model.transform(InferShapes())\n",
    "finn_model = finn_model.transform(InferDataTypes())\n",
    "finn_model = finn_model.transform(GiveUniqueNodeNames())\n",
    "finn_model = finn_model.transform(GiveReadableTensorNames())\n",
    "finn_model.cleanup()\n",
    "\n",
    "# extract input quantization thresholds for sw-based quantization\n",
    "# (in case they were not fixed before training)\n",
    "input_mt_node = finn_model.get_nodes_by_op_type(\"MultiThreshold\")[0]\n",
    "input_mt_thresholds = finn_model.get_initializer(input_mt_node.input[1])\n",
    "print(\"input quant thresholds\")\n",
    "print(input_mt_thresholds)\n",
    "\n",
    "# preprocessing: remove input reshape/quantization from graph\n",
    "new_input_node = finn_model.get_nodes_by_op_type(\"Conv\")[0]   # <---- Change this to the name of the node that has its input node the new desired input graph node.\n",
    "new_input_tensor = finn_model.get_tensor_valueinfo(new_input_node.input[0]) #<--- Get the input node that lead to Conv node or whatever node you just changed above\n",
    "old_input_tensor = finn_model.graph.input[0]  # <--- Get the current starting node \n",
    "finn_model.graph.input.remove(old_input_tensor) #<--- Remove the current starting node\n",
    "finn_model.graph.input.append(new_input_tensor) #<--- Make the [Node before the Conv node] the starting node\n",
    "new_input_index = finn_model.get_node_index(new_input_node)\n",
    "del finn_model.graph.node[0:new_input_index] #<--- Delete every node before the new starting node\n",
    "\n",
    "#We dont have softmax node but still run it anyway\n",
    "# postprocessing: remove final softmax node from training\n",
    "softmax_node = finn_model.graph.node[-1]\n",
    "softmax_in_tensor = finn_model.get_tensor_valueinfo(softmax_node.input[0])\n",
    "softmax_out_tensor = finn_model.get_tensor_valueinfo(softmax_node.output[0])\n",
    "finn_model.graph.output.remove(softmax_out_tensor)\n",
    "finn_model.graph.output.append(softmax_in_tensor)\n",
    "finn_model.graph.node.remove(softmax_node)\n",
    "\n",
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "# insert topK node in place of the final softmax node\n",
    "# topK plays similar role to softmax\n",
    "# k=1 means it pick only 1 class with highest predictions value\n",
    "finn_model = finn_model.transform(InsertTopK(k=1))\n",
    "\n",
    "# manually set input datatype (not done by brevitas yet)\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finn_model.set_tensor_datatype(finnonnx_in_tensor_name, DataType[\"INT8\"])\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Input tensor datatype: %s\" % str(finn_model.get_tensor_datatype(finnonnx_in_tensor_name)))\n",
    "\n",
    "# save modified model that is now ready for the FINN compiler\n",
    "finn_model.save(f\"{build_dir}/radio_27ml_tidy.onnx\")\n",
    "print(\"Modified FINN-ready model saved to %s\" % f\"{build_dir}/radio_27ml_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ac878-e251-420c-9125-6b7b0eb5bc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cb9a4-4561-406f-ba69-100222be5d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
