{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming quantized model prepared for running on FPGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting QONNX format to FINN-QONNX format\n",
    "\n",
    "QONNX format is from the Brevitas library. To run transformation through FINN, we need to convert the model to a FINN-QONNX format.\n",
    "\n",
    "Whenever we want to instantiate an onnx model, we can use `ModelWrapper(filepath)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the brevitas QONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phu/Vivado/Vivado/2024.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phu/Vivado/Vitis_HLS/2024.1\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.util.visualization import showInNetron\n",
    "#Path to the qonnx model exported by brevitas\n",
    "brevitas_model_pth='27ml_rf/models/radio_27ml_export.onnx'\n",
    "\n",
    "model=ModelWrapper(brevitas_model_pth)\n",
    "\n",
    "!echo $VIVADO_PATH\n",
    "!echo $HLS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Brevitas ONNX model using `showInNetron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '27ml_rf/models/radio_27ml_export.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fefb8d0f130>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_machine_ip='your host machine IP'\n",
    "assert host_machine_ip!='your host machine IP', print('host_machine_ip not set')\n",
    "showInNetron(brevitas_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the model to FINN-ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "finn_model_pth='27ml_rf/models/radio_27ml_finn.onnx'\n",
    "model.save(finn_model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing FINN-QONNX model. \n",
    "Notice how it is much easier to read compared to Brevitas ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fefb8d0df30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(finn_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Surgery\n",
    "From the graph above, you can see that the `input` goes through `MultiThreshold`, then `Add`, then `Conv` nodes. The problem we are facing is that FINN is unable to streamline the first `MultiThreshold` node. This is why we need to remove them manually.\n",
    "\n",
    "Originally\n",
    "\n",
    "`input`--> `MultiThreshold`--> `Add`--> `Conv` -->...\n",
    "\n",
    "After network surgery:\n",
    "\n",
    "`Add`--> `Conv` --> ...\n",
    "\n",
    "However, during training, the input of `Add` node expects the output of `MultiThreshold`. Therefore, instead of passing through the raw data to our new model, we will manually perform what `MultiThreshold` does to our raw data (quantizing data) outside the model, then pass it through the new model as input.\n",
    "\n",
    "\n",
    "Further information can be found in this finn's discussion: https://github.com/Xilinx/finn/discussions/420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidying up the model\n",
    "This step does not change parameters in the model, it is purely for reformatting the labels and renaming layers name for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_pre_nw_surgery.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fefb8d0f040>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.general import (\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    ")\n",
    "import onnx\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "# tidy up\n",
    "finn_model = ModelWrapper(finn_model_pth)\n",
    "\n",
    "finn_model = finn_model.transform(InferShapes())\n",
    "finn_model = finn_model.transform(InferDataTypes())\n",
    "finn_model = finn_model.transform(GiveUniqueNodeNames())\n",
    "finn_model = finn_model.transform(GiveReadableTensorNames())\n",
    "finn_model.cleanup()\n",
    "\n",
    "pre_net_surgery_pth='27ml_rf/models/radio_27ml_pre_nw_surgery.onnx'\n",
    "finn_model.save(pre_net_surgery_pth)\n",
    "\n",
    "showInNetron(pre_net_surgery_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: Add_0_out0\n",
      "Input tensor shape: [1, 2, 1024]\n",
      "Input tensor datatype: INT8\n",
      "Modified FINN-ready model saved to 27ml_rf/models/radio_27ml_tidy.onnx\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '27ml_rf/models/radio_27ml_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://10.63.7.166:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fefb8d76260>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finn_model=ModelWrapper(pre_net_surgery_pth)\n",
    "\n",
    "#Find the first 'Conv' node and store it in 'new_input_node'\n",
    "new_input_node = finn_model.get_nodes_by_op_type(\"Conv\")[0]   \n",
    "#Find the input of that 'Conv' node (in this case it is the 'Add' node)\n",
    "new_input_tensor = finn_model.get_tensor_valueinfo(new_input_node.input[0]) \n",
    "\n",
    "#Find the original input node of the model.\n",
    "old_input_tensor = finn_model.graph.input[0] \n",
    "\n",
    "#Remove the old input node, and replace it with the new input tensor ('Add' node)\n",
    "finn_model.graph.input.remove(old_input_tensor) \n",
    "finn_model.graph.input.append(new_input_tensor)\n",
    "\n",
    "#Find the index of the new input node, and remove everything from index 0 to that index\n",
    "#In this case, we will be removing index 0 and index 1, which are the 'inp' and 'MultiThreshold' nodes\n",
    "#So now, the 'Add' node become the model input with index 0, and the 'Conv' node has index 1, and so on...\n",
    "new_input_index = finn_model.get_node_index(new_input_node)\n",
    "del finn_model.graph.node[0:new_input_index]\n",
    "\n",
    "# postprocessing: remove final softmax node from training\n",
    "# Removing any softmax node if there is any. This is because finn will use topK instead of softmax\n",
    "# We have already removed the softmax node when building the model, so this can be ignored\n",
    "softmax_node = finn_model.graph.node[-1]\n",
    "softmax_in_tensor = finn_model.get_tensor_valueinfo(softmax_node.input[0])\n",
    "softmax_out_tensor = finn_model.get_tensor_valueinfo(softmax_node.output[0])\n",
    "finn_model.graph.output.remove(softmax_out_tensor)\n",
    "finn_model.graph.output.append(softmax_in_tensor)\n",
    "finn_model.graph.node.remove(softmax_node)\n",
    "\n",
    "# remove redundant value_info for primary input/output\n",
    "# othwerwise, newer FINN versions will not accept the model\n",
    "if finn_model.graph.input[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.input[0])\n",
    "if finn_model.graph.output[0] in finn_model.graph.value_info:\n",
    "    finn_model.graph.value_info.remove(finn_model.graph.output[0])\n",
    "\n",
    "# insert topK node in place of the final softmax node\n",
    "# topK plays similar role to softmax\n",
    "# k=1 means it pick only 1 classification with highest predictions value\n",
    "finn_model = finn_model.transform(InsertTopK(k=1))\n",
    "\n",
    "# manually set input datatype (not done by brevitas yet)\n",
    "finnonnx_in_tensor_name = finn_model.graph.input[0].name\n",
    "finnonnx_model_in_shape = finn_model.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finn_model.set_tensor_datatype(finnonnx_in_tensor_name, DataType[\"INT8\"])\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Input tensor datatype: %s\" % str(finn_model.get_tensor_datatype(finnonnx_in_tensor_name)))\n",
    "\n",
    "# save modified model that is now ready for the FINN compiler\n",
    "tidy_model_pth='27ml_rf/models/radio_27ml_tidy.onnx'\n",
    "finn_model.save(tidy_model_pth)\n",
    "print(\"Modified FINN-ready model saved to %s\" % tidy_model_pth)\n",
    "\n",
    "showInNetron(tidy_model_pth,localhost_url=host_machine_ip, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINN Transformation\n",
    "From here, we will setup a FINN's standard builder, with a few custom transformations, and export a bitfile which can be run using pynq on the FPGA.\n",
    "\n",
    "Original version of the code below can be found here: [Original version](https://github.com/Xilinx/finn-examples/blob/main/build/vgg10-radioml/README.md)\n",
    "\n",
    "Further information about setting up a builder for transformations can be found here: [Tutorial](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/cybersecurity/3-build-accelerator-with-finn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom steps for the builder\n",
    "The builder has a few steps already prepared for us. However, since we are using a 1D conv layer, we will need to add 2 more custom steps to convert them from 1D to 2D\n",
    "\n",
    "`step_pre_streamline` is for converting from our model from 3D tensors to 4D tensors. This is because we initially use 1D convolutional layers. This means the input shape will be changed from `1x2x1024` to `1x1x2x1024`\n",
    "\n",
    "`step_convert_final_layers` is for converting the final layers (linear and topK) to hardware layers\n",
    "\n",
    "Further discussion about 1D conversion to 2D: [Github Discussion](https://github.com/Xilinx/finn/discussions/418)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.change_3d_tensors_to_4d import Change3DTo4DTensors\n",
    "from qonnx.transformation.general import GiveUniqueNodeNames\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.builder.build_dataflow_config import DataflowBuildConfig\n",
    "from finn.transformation.fpgadataflow.set_fifo_depths import InsertAndSetFIFODepths\n",
    "\n",
    "\n",
    "def step_pre_streamline(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    model = model.transform(Change3DTo4DTensors())\n",
    "    model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "    return model\n",
    "\n",
    "\n",
    "def step_convert_final_layers(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    model = model.transform(to_hw.InferChannelwiseLinearLayer())\n",
    "    model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "from finn.util.basic import alveo_default_platform\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\"--------------------------------------------------------------------\"\n",
    "#Get the tidy.onnx model\n",
    "model_name='radio_27ml_tidy'\n",
    "model_file = tidy_model_pth\n",
    "\n",
    "#Create a temporary folders (if none exist) to store intermediate transformations\n",
    "finn_build_dir=os.getcwd()+'/tmp/'\n",
    "os.environ[\"FINN_BUILD_DIR\"]=finn_build_dir\n",
    "\n",
    "final_output_dir=\"output/\"\n",
    "Path(finn_build_dir).mkdir(exist_ok=True)\n",
    "Path(final_output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "#Remove all intermediate transformations from previous runs \n",
    "print(f'temp files will be built in {finn_build_dir}')\n",
    "print(f'removing old temp files in {finn_build_dir}')\n",
    "files = glob.glob(f'{finn_build_dir}*')\n",
    "for f in files:\n",
    "    if os.path.isdir(f):\n",
    "        shutil.rmtree(f)\n",
    "    elif os.path.isfile(f):\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "print(f'output will be generated in {final_output_dir}')\n",
    "\"--------------------------------------------------------------------\"\n",
    "\n",
    "# which platforms to build the networks for\n",
    "zynq_platforms = [\"ZCU104\"]\n",
    "alveo_platforms = []\n",
    "platforms_to_build = zynq_platforms + alveo_platforms\n",
    "\n",
    "\n",
    "# determine which shell flow to use for a given platform\n",
    "def platform_to_shell(platform):\n",
    "    if platform in zynq_platforms:\n",
    "        return build_cfg.ShellFlowType.VIVADO_ZYNQ\n",
    "    elif platform in alveo_platforms:\n",
    "        return build_cfg.ShellFlowType.VITIS_ALVEO\n",
    "    else:\n",
    "        raise Exception(\"Unknown platform, can't determine ShellFlowType\")\n",
    "\n",
    "\n",
    "# select target clock frequency\n",
    "def select_clk_period(platform):\n",
    "    return 4.0\n",
    "    #return 5.0\n",
    "\n",
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "# assemble build flow from custom and pre-existing steps\n",
    "def select_build_steps(platform):\n",
    "    return [\n",
    "        \"step_tidy_up\",\n",
    "        step_pre_streamline, #Custom steps above\n",
    "        \"step_streamline\",\n",
    "        \"step_convert_to_hw\",\n",
    "        step_convert_final_layers,  #Custom steps above\n",
    "        \"step_create_dataflow_partition\",\n",
    "        \"step_specialize_layers\",\n",
    "        \"step_target_fps_parallelization\",\n",
    "        \"step_apply_folding_config\",\n",
    "        \"step_minimize_bit_width\",\n",
    "        \"step_generate_estimate_reports\",\n",
    "        \"step_hw_codegen\",\n",
    "        \"step_hw_ipgen\",\n",
    "        \"step_set_fifo_depths\",\n",
    "        \"step_create_stitched_ip\",\n",
    "        \"step_measure_rtlsim_performance\",\n",
    "        \"step_out_of_context_synthesis\",\n",
    "        \"step_synthesize_bitfile\",\n",
    "        \"step_make_pynq_driver\",\n",
    "        \"step_deployment_package\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# create a release dir, used for finn-examples release packaging\n",
    "os.makedirs(\"release\", exist_ok=True)\n",
    "\n",
    "for platform_name in platforms_to_build:\n",
    "    shell_flow_type = platform_to_shell(platform_name)\n",
    "    if shell_flow_type == build_cfg.ShellFlowType.VITIS_ALVEO:\n",
    "        vitis_platform = alveo_default_platform[platform_name]\n",
    "        # for Alveo, use the Vitis platform name as the release name\n",
    "        # e.g. xilinx_u250_xdma_201830_2\n",
    "        release_platform_name = vitis_platform\n",
    "    else:\n",
    "        vitis_platform = None\n",
    "        # for Zynq, use the board name as the release name\n",
    "        # e.g. ZCU104\n",
    "        release_platform_name = platform_name\n",
    "    platform_dir = \"release/%s\" % release_platform_name\n",
    "    os.makedirs(platform_dir, exist_ok=True)\n",
    "\n",
    "    cfg = build_cfg.DataflowBuildConfig(\n",
    "        steps=select_build_steps(platform_name),\n",
    "        output_dir=final_output_dir+\"output_%s_%s\" % (model_name, release_platform_name),\n",
    "        synth_clk_period_ns=select_clk_period(platform_name),\n",
    "        target_fps=4600, #Target FPS, not guaranteed the model will achieve\n",
    "        board=platform_name,\n",
    "        shell_flow_type=shell_flow_type,\n",
    "        vitis_platform=vitis_platform,\n",
    "        split_large_fifos=True,\n",
    "        standalone_thresholds=True,\n",
    "        # enable extra performance optimizations (physopt)\n",
    "        vitis_opt_strategy=build_cfg.VitisOptStrategyCfg.PERFORMANCE_BEST,\n",
    "        generate_outputs=[\n",
    "            build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "            build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "            # build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "            build_cfg.DataflowOutputType.BITFILE,\n",
    "            build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "            build_cfg.DataflowOutputType.PYNQ_DRIVER,\n",
    "        ],\n",
    "        \n",
    "    )\n",
    "    build.build_dataflow_cfg(model_file, cfg)\n",
    "\n",
    "    # copy bitfiles and runtime weights into release dir if found\n",
    "    bitfile_gen_dir = cfg.output_dir + \"/bitfile\"\n",
    "    files_to_check_and_copy = [\n",
    "        \"finn-accel.bit\",\n",
    "        \"finn-accel.hwh\",\n",
    "        \"finn-accel.xclbin\",\n",
    "    ]\n",
    "    for f in files_to_check_and_copy:\n",
    "        src_file = bitfile_gen_dir + \"/\" + f\n",
    "        dst_file = platform_dir + \"/\" + f.replace(\"finn-accel\", model_name)\n",
    "        if os.path.isfile(src_file):\n",
    "            shutil.copy(src_file, dst_file)\n",
    "\n",
    "    weight_gen_dir = cfg.output_dir + \"/driver/runtime_weights\"\n",
    "    weight_dst_dir = platform_dir + \"/%s_runtime_weights\" % model_name\n",
    "    if os.path.isdir(weight_gen_dir):\n",
    "        weight_files = os.listdir(weight_gen_dir)\n",
    "        if weight_files:\n",
    "            shutil.copytree(weight_gen_dir, weight_dst_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
