{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio Modulation Classification with FINN - Notebook #2 of 5\n",
    "\n",
    "### Overview \n",
    "In this Notebook we again load the dataset and create our dataloader. Then we:\n",
    "1. Define our model using Brevitas\n",
    "2. Train, and test our new Brevitas model!\n",
    "\n",
    "Portions of this notebook that have been covered in the previous notebook will have much less description (such as the dataloader). The more information please see the Tutorial1_Dataset_and_Vanilla_model.ipynb file!\n",
    "\n",
    "### FINN Pipeline Map\n",
    "Throughout these notebooks, you will begin to understand the FINN pipeline! In order the pipeline is:\n",
    "1. Dataset and Vanilla model\n",
    "2. **Brevitas Model** (you are here)\n",
    "3. Transforming the Brevitas Model to tidy.onnx\n",
    "4. Transforming tidy.onnx to bitstream\n",
    "5. Loading the bitstream on the FPGA!\n",
    "\n",
    "We are in **2. Brevitas Model.** This notebook will show you how to define a VGG-10 model using the Brevitas Framework, which allows us to quantize the model (i.e make a smaller model). Having a quantized model is required for the next steps of the pipeline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages, and Data \n",
    "First we import the required packages and create the dataloader for the dataset. The details of this are explained in Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required pacakages \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cuda \n",
    "assert torch.cuda.is_available(), 'Cuda not available'\n",
    "gpu = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if dataset is present\n",
    "import os.path\n",
    "dataset_path = \"datasets/RADIOML_2021_07_INT8.hdf5\"\n",
    "os.path.isfile(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "class radioml_21_dataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        super(radioml_21_dataset, self).__init__()\n",
    "        h5_file = h5py.File(dataset_path,'r')\n",
    "        self.data = h5_file['X']\n",
    "        self.mod = np.argmax(h5_file['Y'], axis=1) # comes in one-hot encoding\n",
    "        self.snr = h5_file['Z'][:,0]\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "        self.mod_classes = [\n",
    "                \"OOK\",\n",
    "                \"4ASK\",\n",
    "                \"8ASK\",\n",
    "                \"BPSK\",\n",
    "                \"QPSK\",\n",
    "                \"8PSK\",\n",
    "                \"16PSK\",\n",
    "                \"32PSK\",\n",
    "                \"16APSK\",\n",
    "                \"32APSK\",\n",
    "                \"64APSK\",\n",
    "                \"128APSK\",\n",
    "                \"16QAM\",\n",
    "                \"32QAM\",\n",
    "                \"64QAM\",\n",
    "                \"128QAM\",\n",
    "                \"256QAM\",\n",
    "                \"AM-SSB-WC\",\n",
    "                \"AM-SSB-SC\",\n",
    "                \"AM-DSB-WC\",\n",
    "                \"AM-DSB-SC\",\n",
    "                \"FM\",\n",
    "                \"GMSK\",\n",
    "                \"OQPSK\",\n",
    "                \"BFSK\",\n",
    "                \"4FSK\",\n",
    "                \"8FSK\",\n",
    "            ]\n",
    "        self.num_classes=len(self.mod_classes)\n",
    "        # print(np.unique(self.snr))\n",
    "        # print(self.data.shape)\n",
    "        # print(np.min(self.data),'   ',np.max(self.data),'  ',self.data.dtype)\n",
    "        self.snr_classes = np.arange(-20., 32., 2) # -20dB to 30dB, with step of 2 --> 26 snrs\n",
    "\n",
    "        # do not touch this seed to ensure the prescribed train/test split!\n",
    "        np.random.seed(2021)\n",
    "\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        for mod in range(0, len(self.mod_classes)): # all modulations (0 to 26)\n",
    "            for snr_idx in range(0, 26): # all SNRs (0 to 25 = -20dB to +30dB)\n",
    "                # raw dataset holds frames strictly ordered by modulation and SNR\n",
    "                # We order the dataset to each mod-snr pair combination for better access of each frame\n",
    "                # Specifically we divide the dataset into 27 mods group,\n",
    "                #   and for each group we divide into 26 SNRs,\n",
    "                # For each modulation-snr pair combination, we have 4096 frames. (27*26*4096 = 2875392)\n",
    "                # For better analogy, its basically a triple for-loop, with the outer most loop being 27 mods,\n",
    "                #                                                           then the middle being 26 SNRs,\n",
    "                #                                                           then inner most being 4096 samples\n",
    "                # Basically [0[0[0...4095] ...25]...26] with a length of 2875392\n",
    "                start_idx = 26*4096*mod + 4096*snr_idx \n",
    "                indices_subclass = list(range(start_idx, start_idx+4096))\n",
    "                \n",
    "                # 90%/10% training/test split, applied evenly for each mod-SNR pair\n",
    "                split = int(np.ceil(0.1 * 4096)) \n",
    "                np.random.shuffle(indices_subclass)\n",
    "                train_indices_subclass = indices_subclass[split:]\n",
    "                test_indices_subclass = indices_subclass[:split]\n",
    "                \n",
    "                # you could train on a subset of the data, e.g. based on the SNR\n",
    "                # here we use all available training samples\n",
    "                if snr_idx >= 0:\n",
    "                    train_indices.extend(train_indices_subclass)\n",
    "                test_indices.extend(test_indices_subclass)\n",
    "                \n",
    "        self.train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        self.test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # transpose frame into Pytorch channels-first format (NCL = -1,2,1024)\n",
    "        return self.data[idx].transpose(), self.mod[idx], self.snr[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset = radioml_21_dataset(dataset_path) \n",
    "\n",
    "print('Value range: ', np.min(dataset.data),'   ',np.max(dataset.data),'  ',dataset.data.dtype) #The total range of int8 is [-127,128]\n",
    "print('Total mods: ',dataset.num_classes)\n",
    "print('Number of SNRs: ',len(dataset.snr_classes))\n",
    "print('Number of frames per each SNR-Modulation combination: ',dataset.data.shape[0]/(dataset.num_classes*len(dataset.snr_classes)))\n",
    "print('SNRs: ',dataset.snr_classes,' \\n')\n",
    "print('Total size: ',dataset.data.shape)\n",
    "print('Training set size: ',len(dataset.train_sampler))\n",
    "print('Test/Val set size: ',len(dataset.test_sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the VGG-10 model\n",
    "\n",
    "This model is built with Brevitas (and Pytorch for non-parameters layers), meaning this model has already been quantized before training. \n",
    "The structure of the model follows the VGG-10 model described in this paper by DeepSig: [Over-the-Air Deep Learning Based Radio Signal Classification](https://arxiv.org/pdf/1712.04578.pdf).\n",
    "\n",
    "The model should have: \n",
    "- Input quantizer layer to convert from int8 to a float tensor, which can be forwarded into QuantConv2d \n",
    "- 7 convolutional blocks, \n",
    "- 1 flatten layer,\n",
    "- 2 linear blocks \n",
    "- 1 final linear layer for classification\n",
    "\n",
    "Further explanation on quant tensor: https://github.com/Xilinx/brevitas/blob/master/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb \n",
    "\n",
    "\n",
    "## TODO - From Ryan \n",
    "Brevitas, just like Vitis, is under ALOT of active development which is a good thing! But also means our implementation here looks out of\n",
    "date compared to what they have now. Typically with new Pytorch versions, we do not necessarily need to stay up to date. However, the \n",
    "new API for Brevitas looks much better, or like normal pytorch and is more understandable, than the API we use here. \n",
    "\n",
    "However before thats done I think it's important we have the full pipeline udpated and good to go before doing so. \n",
    "\n",
    "Here's the link to the Brevitas Docs, but beware they are 90% incomplete... the developer has TODOs all around the website... \n",
    "\n",
    "\n",
    "## Brevitas Quantization at a Highlevel \n",
    "\n",
    "Brevitas _heavily abstracts_ the idea of quantizing and therefore gives us a lot of customization options! By definition quantizing is performing _some_ operation on a value to reduce it's precision. More informally, given some value that takes A bits to represent, change it to a new value that uses less than A bits to represent. \n",
    "\n",
    "Therefore, Brevitas is built upon the idea of defining **Custom Quantizers**. The quantizers are modules that allow the user to:\n",
    "1. Select which parts {Weights, Bias, Input, Outputs} of a layer get quantized\n",
    "2. The method by which each part of each layer is quantized {Custom, or defualts such as power of 2 quantization, scaled uniform, etc. brevitas defines _alot_ of quantization types}\n",
    "\n",
    "Specifically, we can define 3 custom modules {WeightQuantType, BiasQuantType, ActQuantType}. The first two are used to quantize the Weights and Bias respectively, and the last one is used to quantize inputs and outputs of a layer. \n",
    "\n",
    "\n",
    "For example, below we use `Int8ActPerTensorFloatMinMaxInit` to define a ActQuantizer. This quantizer takes 3 parameters: bit_width, min_val, max_val. This will force corresponding tensors to have values between the min_val and max_val, and will be represented with bit_width bits.  \n",
    "\n",
    "Formally, we define our quantizer as `InputQuantizer`, which inherits from `Int8ActPerTensorFloatMinMaxInit`. Meaning we can now use our quantizer **to enforce our quantizing rules** on a new layer! While verbose, you may now see the power we have to define very custom quantizers. \n",
    "\n",
    "Here is a tutorial to make your own quantizer: https://xilinx.github.io/brevitas/tutorials/anatomy_quantizer.html. \n",
    "\n",
    "TODO: For Ryan and or other: Difference between a quantizer and the quantizer activation. Once I/someone else knows better we can add that here. \n",
    "\n",
    "\n",
    "#### Specifics \n",
    "\n",
    "**QuantRelu:** Implements a standard ReLu layer _followed_ by quantization \n",
    "\n",
    "**QuantConv1d:** This is an instance of both a standard Conv2d layer and QuantWeightBiasInputOutputLayer (QuantWBIOL) layer. A WBIOL layer is a layer that **allows for** quantizing of Weights, Bias, Inputs, and Outputs. Allow for, meaning, it does not by default enable quantization for all those layers, but if a user so wishes, they can choose to enable quantization for all those layers \n",
    "\n",
    "**QuantLinear:** Another WBIOL layer, this time combined with a Linear Layer. \n",
    "\n",
    "\n",
    "The default type of quantization for these layers is: Int8WeightPerTensorFloat, meaning the Wieghts are quantized to 8bit floats. However you can see in our implementation we specify a `weight_bit_width` and a `bias`. So we explicity set bias to false (which is redundant but good practice) and set weight_bit_width to a value that we can now customize! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# A qnn is a Brevitas version of pytorch's nn. nn stands for neural network.\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "\n",
    "from brevitas.quant import Int8Bias\n",
    "\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "# Adjustable hyperparameters\n",
    "input_bits = 8\n",
    "a_bits = 8  # a_bits is the bit width for ReLu\n",
    "w_bits = 8 # w_bits is the bit width for all the weights\n",
    "filters_conv = 64\n",
    "filters_dense = 128\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    #Using 8 bits\n",
    "    bit_width = input_bits\n",
    "    #Converting from int8 [-127,128] to finn-float32 [-127.0,128.0]\n",
    "    #Since our dataset is already quantized before going through the model,\n",
    "    #   this would only convert the datatype, the values would be the same\n",
    "    #Refer to this notebook below, which is used for the 2018 dataset,\n",
    "    #to see the difference between this version and the 2018 version\n",
    "    #(https://github.com/Xilinx/brevitas-radioml-challenge-21/blob/main/sandbox/notebooks/training_and_evaluation.ipynb)\n",
    "    min_val = -127.0\n",
    "    max_val = 128.0\n",
    "\n",
    "    # \n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "model_class = nn.Sequential(\n",
    "    # Input quantization layer\n",
    "    qnn.QuantHardTanh(act_quant=InputQuantizer),\n",
    "\n",
    "    qnn.QuantConv1d(2, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits,bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    qnn.QuantLinear(filters_conv*8, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits, return_quant_tensor=True),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, 27, weight_bit_width=w_bits, bias=True, bias_quant=Int8Bias),\n",
    ")\n",
    "model=model_class\n",
    "\n",
    "import torchinfo\n",
    "print(torchinfo.summary(model_class,input_size=(1,2,1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining train, test, displaying loss plot graph function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "\n",
    "    #TODO: TEMP\n",
    "    c = 0 \n",
    "    \n",
    "    for (inputs, target, snr) in tqdm(train_loader, desc=\"Training Batches\"):#, leave=False):   \n",
    "        #if gpu is not None:\n",
    "        inputs = inputs.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "                \n",
    "        # forward pass\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        c+=1 \n",
    "        print(c)\n",
    "        if c > 15:\n",
    "            return losses\n",
    "           \n",
    "    return losses\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for (inputs, target, snr) in tqdm(test_loader, desc=\"Testing Batches\", leave=False):\n",
    "            #if gpu is not None:\n",
    "            inputs = inputs.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            output = model(inputs)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a path to store the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='27ml_rf'\n",
    "model_file_name='27ml_rf_quantized.pth'\n",
    "\n",
    "Path(model_name).mkdir(exist_ok=True)\n",
    "chpt_path=model_name+'/'+model_file_name\n",
    "print(f'Model parameters will be saved in {chpt_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are training the model for 20 epochs with a batch size of 1024 by default.\n",
    "These numbers can be adjusted.\n",
    "\n",
    "The first epoch usually takes longer to train. \n",
    "After that, it should train faster per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 20\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size, sampler=dataset.train_sampler)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)\n",
    "\n",
    "model = model.to(gpu)\n",
    "\n",
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(gpu)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "training_time=time.time()\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        torch.save(model.state_dict(), chpt_path)\n",
    "        print(f'Model checkpoint is saved in {chpt_path}')\n",
    "\n",
    "        test_acc = test(model, data_loader_test)\n",
    "        print(\"Epoch %d: Training loss = %f, validation accuracy = %f\" % (epoch, np.mean(loss_epoch), test_acc))\n",
    "\n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)\n",
    "        lr_scheduler.step()\n",
    "training_time=time.time()-training_time\n",
    "print(f'total training time: {training_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over epochs\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test accuracy over epochs\n",
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained parameters to disk\n",
    "torch.save(model.state_dict(), chpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the model back again, or you can change the file to load a different model\n",
    "#Redefining the model class because we only save the parameters, not the structure of the model\n",
    "load_path=chpt_path #Change this to a path of a different model\n",
    "model=model_class #Reinitialize the class of the model, so we can fill the parameters in\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a fresh test data loader\n",
    "batch_size = 1024\n",
    "dataset = radioml_21_dataset(dataset_path)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7202f196492045b9ad79dcedb8d1144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run inference on validation data\n",
    "y_exp = np.empty((0))\n",
    "y_snr = np.empty((0))\n",
    "y_pred = np.empty((0,len(dataset.mod_classes)))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader_test, desc=\"Batches\"):\n",
    "        inputs, target, snr = data\n",
    "        inputs = inputs.to(gpu).float()\n",
    "        output = model(inputs)\n",
    "        y_pred = np.concatenate((y_pred,output.cpu()))\n",
    "        y_exp = np.concatenate((y_exp,target))\n",
    "        y_snr = np.concatenate((y_snr,snr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix across all SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall confusion matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=90)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "for i in range(len(y_exp)):\n",
    "    j = int(y_exp[i])\n",
    "    k = int(np.argmax(y_pred[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(dataset.mod_classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_confusion_matrix(confnorm, labels=dataset.mod_classes)\n",
    "\n",
    "cor = np.sum(np.diag(conf))\n",
    "ncor = np.sum(conf) - cor\n",
    "print(\"Overall Accuracy across all SNRs: %f\"%(cor / (cor+ncor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix at 4 specific SNRs\n",
    "Notice how the accuracy is very low at lower SNR and the accuracy is very high at higher SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices at 4 different SNRs\n",
    "snr_to_plot = [-20,-4,+4,+30]\n",
    "plt.figure(figsize=(16,10))\n",
    "acc = []\n",
    "for snr in dataset.snr_classes:\n",
    "    # extract classes @ SNR\n",
    "    indices_snr = (y_snr == snr).nonzero()\n",
    "    y_exp_i = y_exp[indices_snr]\n",
    "    y_pred_i = y_pred[indices_snr]\n",
    " \n",
    "    conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    for i in range(len(y_exp_i)):\n",
    "        j = int(y_exp_i[i])\n",
    "        k = int(np.argmax(y_pred_i[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(dataset.mod_classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    " \n",
    "    if snr in snr_to_plot:\n",
    "        plot, = np.where(snr_to_plot == snr)[0]\n",
    "        plt.subplot(221+plot)\n",
    "        plot_confusion_matrix(confnorm, labels=dataset.mod_classes, title=\"Confusion Matrix @ %d dB\"%(snr))\n",
    " \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    acc.append(cor/(cor+ncor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy over SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over SNR\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(dataset.snr_classes, acc, marker='o')\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.xlim([-20, 30])\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title(\"Classification Accuracy over SNR\")\n",
    "plt.grid()\n",
    "plt.title(\"Classification Accuracy over SNR\");\n",
    "\n",
    "print(\"Accuracy @ highest SNR (+30 dB): %f\"%(acc[-1]))\n",
    "print(\"Accuracy overall: %f\"%(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of the accuracy of each modulations over SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy per modulation\n",
    "accs = []\n",
    "for mod in range((dataset.num_classes)):\n",
    "    accs.append([])\n",
    "    for snr in dataset.snr_classes:\n",
    "        indices = ((y_exp == mod) & (y_snr == snr)).nonzero()\n",
    "        y_exp_i = y_exp[indices]\n",
    "        y_pred_i = y_pred[indices]\n",
    "        cor = np.count_nonzero(y_exp_i == np.argmax(y_pred_i, axis=1))\n",
    "        accs[mod].append(cor/len(y_exp_i))\n",
    "        \n",
    "# Plot accuracy-over-SNR curve\n",
    "plt.figure(figsize=(12,8))\n",
    "for mod in range(dataset.num_classes):\n",
    "    if accs[mod][-1] < 0.95 or accs[mod][0] > 0.1:\n",
    "        color = None\n",
    "    else:\n",
    "        color = \"black\"\n",
    "    plt.plot(dataset.snr_classes, accs[mod], label=str(mod) + \": \" + dataset.mod_classes[mod], color=color)\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.xlim([-20, 30])\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title(\"Accuracy breakdown\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model as QONNX (Quantized ONNX) file\n",
    "\n",
    "QONNX is just a extended version of ONNX file. \n",
    "\n",
    "QONNX file is still saved under the extension `.onnx`, but their parameters are quantized\n",
    "\n",
    "To standardize the project, we will call:\n",
    "- QONNX Model exported from Brevitas: `_export.onnx`\n",
    "- QONNX Model converted to FINN-ONNX: `_finn.onnx`\n",
    "- QONNX Model after network surgery: `_tidy.onnx`\n",
    "\n",
    "Futher information about FINN-ONNX and network surgery is explained in the `build_transformed_model.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "model.eval()\n",
    "build_dir=\"27ml_rf/models\" #Directory to save model\n",
    "#Ensuring path exist, otherwise create an empty directory\n",
    "Path(build_dir).mkdir(exist_ok=True)\n",
    "export_path=f\"{build_dir}/radio_27ml_export.onnx\" #Full name of the path of the model with the tail _export.onnx\n",
    "export_qonnx(model.to('cuda'), torch.randn(1, 2, 1024).to('cuda'), export_path=export_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the model so that FINN can regconize nodes\n",
    "\n",
    "Ensure the model is cleaned up before converting to FINN ONNX, so that the nodes in the model are correctly labeled. Otherwise, when we convert to FINN-ONNX, the converter may not be able to regconize the nodes.\n",
    "\n",
    "Source code for cleanup(): https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/util/cleanup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "qonnx_cleanup(export_path, out_file=export_path)\n",
    "print(f'model is saved in {export_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
