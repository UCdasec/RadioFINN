{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio Modulation Classification with FINN - Notebook #2 of 5\n",
    "\n",
    "### Overview \n",
    "In this Notebook we again load the dataset and create our dataloader. Then we:\n",
    "1. Define our model using Brevitas\n",
    "2. Train, and test our new Brevitas model!\n",
    "\n",
    "Portions of this notebook that have been covered in the previous notebook will have much less description (such as the dataloader). The more information please see the Tutorial1_Dataset_and_Vanilla_model.ipynb file!\n",
    "\n",
    "### FINN Pipeline Map\n",
    "Throughout these notebooks, you will begin to understand the FINN pipeline! In order the pipeline is:\n",
    "1. Dataset and Vanilla model\n",
    "2. **Brevitas Model** (you are here)\n",
    "3. Transforming the Brevitas Model to tidy.onnx\n",
    "4. Transforming tidy.onnx to bitstream\n",
    "5. Loading the bitstream on the FPGA!\n",
    "\n",
    "We are in **2. Brevitas Model.** This notebook will show you how to define a VGG-10 model using the Brevitas Framework, which allows us to quantize the model (i.e make a smaller model). Having a quantized model is required for the next steps of the pipeline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages, and Data \n",
    "First we import the required packages and create the dataloader for the dataset. The details of this are explained in Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required pacakages \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cuda \n",
    "assert torch.cuda.is_available(), 'Cuda not available'\n",
    "gpu = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is present\n",
    "import os.path\n",
    "dataset_path = \"datasets/RADIOML_2021_07_INT8.hdf5\"\n",
    "os.path.isfile(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter for dataloader\n",
    "\n",
    "dataset_bit_width=8 #bit width of the raw dataset\n",
    "model_input_bit=8 #bit width of quantized model\n",
    "\n",
    "#If the original and target bit width are the same, then it will skip mapping the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "\n",
    "class radioml_21_dataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        super(radioml_21_dataset, self).__init__()\n",
    "        h5_file = h5py.File(dataset_path,'r')\n",
    "        self.data = h5_file['X'][:] \n",
    "        \n",
    "        print('start quantizing dataset')\n",
    "        self.quantize_bit(dataset_bit_width,model_input_bit)\n",
    "        print('done quantizing dataset')\n",
    "        \n",
    "        self.mod = np.argmax(h5_file['Y'], axis=1) # comes in one-hot encoding\n",
    "        self.snr = h5_file['Z'][:,0]\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "        self.mod_classes = [\n",
    "                \"OOK\",\n",
    "                \"4ASK\",\n",
    "                \"8ASK\",\n",
    "                \"BPSK\",\n",
    "                \"QPSK\",\n",
    "                \"8PSK\",\n",
    "                \"16PSK\",\n",
    "                \"32PSK\",\n",
    "                \"16APSK\",\n",
    "                \"32APSK\",\n",
    "                \"64APSK\",\n",
    "                \"128APSK\",\n",
    "                \"16QAM\",\n",
    "                \"32QAM\",\n",
    "                \"64QAM\",\n",
    "                \"128QAM\",\n",
    "                \"256QAM\",\n",
    "                \"AM-SSB-WC\",\n",
    "                \"AM-SSB-SC\",\n",
    "                \"AM-DSB-WC\",\n",
    "                \"AM-DSB-SC\",\n",
    "                \"FM\",\n",
    "                \"GMSK\",\n",
    "                \"OQPSK\",\n",
    "                \"BFSK\",\n",
    "                \"4FSK\",\n",
    "                \"8FSK\",\n",
    "            ]\n",
    "        self.num_classes=len(self.mod_classes)\n",
    "        self.snr_classes = np.arange(-20., 32., 2) # -20dB to 30dB, with step of 2 --> 26 snrs or [0,25]\n",
    "\n",
    "        np.random.seed(2021)\n",
    "\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        val_indices = []\n",
    "\n",
    "        for mod in range(0, len(self.mod_classes)): # all modulations (0 to 25)\n",
    "            for snr_idx in range(0, 26): # all SNRs (0 to 25 = -20dB to +30dB)\n",
    "                start_idx = 26*4096*mod + 4096*snr_idx \n",
    "                indices_subclass = list(range(start_idx, start_idx+4096))\n",
    "                \n",
    "                # 90%/10% training/test split, applied evenly for each mod-SNR pair\n",
    "                # 80 10 10 split \n",
    "                split = int(np.ceil(0.8 * 4096)) \n",
    "                split2 = int(np.ceil(0.9 * 4096)) \n",
    "\n",
    "                np.random.shuffle(indices_subclass)\n",
    "                train_indices_subclass = indices_subclass[:split]\n",
    "                val_indices_subclass = indices_subclass[split:split2]\n",
    "                test_indices_subclass = indices_subclass[split2:]\n",
    "                \n",
    "                # you could train on a subset of the data, e.g. based on the SNR\n",
    "                # here we use all available training samples\n",
    "                if snr_idx >= 0:\n",
    "                    train_indices.extend(train_indices_subclass)\n",
    "\n",
    "                test_indices.extend(test_indices_subclass)\n",
    "                val_indices.extend(val_indices_subclass)\n",
    "                \n",
    "        self.train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        self.val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "        self.test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "        \n",
    "    def quantize_bit(self, original_bit_width, target_bit_width):\n",
    "        if original_bit_width==target_bit_width:\n",
    "            print('original_bit_width already the same as target')\n",
    "            return\n",
    "        scale = (2 ** target_bit_width) - 1\n",
    "        zero_center = 2 ** (target_bit_width - 1)\n",
    "        \n",
    "        # Normalize to [0, 1], scale to [0, 2^bit-1], shift to center around 0, then cast\n",
    "        quantized = ((self.data.astype(np.float32) + 2**(original_bit_width-1)) / 2**original_bit_width)\n",
    "        quantized*=scale\n",
    "        quantized = np.round(quantized) - zero_center\n",
    "    \n",
    "        self.data = quantized.astype(np.int8) \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # transpose frame into Pytorch channels-first format (NCL = -1,2,1024)\n",
    "        return self.data[idx].transpose(), self.mod[idx], self.snr[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = radioml_21_dataset(dataset_path) \n",
    "print('Value range: ', np.min(dataset.data),'   ',np.max(dataset.data),'  ',dataset.data.dtype) #The total range of int8 is [-127,128]\n",
    "print('Total mods: ',dataset.num_classes)\n",
    "print('Number of SNRs: ',len(dataset.snr_classes))\n",
    "print('Number of frames per each SNR-Modulation combination: ',dataset.data.shape[0]/(dataset.num_classes*len(dataset.snr_classes)))\n",
    "print('SNRs: ',dataset.snr_classes,' \\n')\n",
    "print('Total size: ',dataset.data.shape)\n",
    "print('Training set size: ',len(dataset.train_sampler))\n",
    "print('Val set size: ',len(dataset.val_sampler))\n",
    "print('Test set size: ',len(dataset.test_sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the VGG-10 model\n",
    "This model is built with Brevitas (and Pytorch for non-parameters layers), meaning this model has already been quantized before training. \n",
    "\n",
    "\n",
    "## Brevitas Quantization at a Highlevel \n",
    "\n",
    "Brevitas _heavily abstracts_ the idea of quantizing and therefore gives us a lot of customization options! By definition quantizing is performing _some_ operation on a value to reduce it's precision. More informally, given some value that takes A bits to represent, change it to a new value that uses less than A bits to represent. \n",
    "\n",
    "Therefore, Brevitas is built upon the idea of defining **Custom Quantizers**. The quantizers are modules that allow the user to:\n",
    "1. Select which parts {Weights, Bias, Input, Outputs} of a layer get quantized\n",
    "2. The method by which each part of each layer is quantized {Custom, or defualts such as power of 2 quantization, scaled uniform, etc. brevitas defines _alot_ of quantization types}\n",
    "\n",
    "Specifically, we can define 3 custom modules {WeightQuantType, BiasQuantType, ActQuantType}. The first two are used to quantize the Weights and Bias respectively, and the last one is used to quantize inputs and outputs of a layer. \n",
    "\n",
    "\n",
    "For example, below we use `Int8ActPerTensorFloatMinMaxInit` to define a ActQuantizer. This quantizer takes 3 parameters: bit_width, min_val, max_val. This will force corresponding tensors to have values between the min_val and max_val, and will be represented with bit_width bits.  \n",
    "\n",
    "Formally, we define our quantizer as `InputQuantizer`, which inherits from `Int8ActPerTensorFloatMinMaxInit`. Meaning we can now use our quantizer **to enforce our quantizing rules** on a new layer! While verbose, you may now see the power we have to define very custom quantizers. \n",
    "\n",
    "Here is a tutorial to make your own quantizer: https://xilinx.github.io/brevitas/tutorials/anatomy_quantizer.html. \n",
    "\n",
    "\n",
    "#### Specifics \n",
    "\n",
    "**QuantRelu:** Implements a standard ReLu layer _followed_ by quantization \n",
    "\n",
    "**QuantConv1d:** This is an instance of both a standard Conv2d layer and QuantWeightBiasInputOutputLayer (QuantWBIOL) layer. A WBIOL layer is a layer that **allows for** quantizing of Weights, Bias, Inputs, and Outputs. Allow for, meaning, it does not by default enable quantization for all those layers, but if a user so wishes, they can choose to enable quantization for all those layers \n",
    "\n",
    "**QuantLinear:** Another WBIOL layer, this time combined with a Linear Layer. \n",
    "\n",
    "\n",
    "The default type of quantization for these layers is: Int8WeightPerTensorFloat, meaning the Wieghts are quantized to 8bit floats. However you can see in our implementation we specify a `weight_bit_width` and a `bias`. So we explicity set bias to false (which is redundant but good practice) and set weight_bit_width to a value that we can now customize! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Adjustable hyperparameters\n",
    "input_bits = model_input_bit #The same bit width as the new loaded dataset\n",
    "a_bits = 8  # a_bits is the bit width for ReLu\n",
    "w_bits = 8 # w_bits is the bit width for all the weights\n",
    "filters_conv = 64\n",
    "filters_dense = 128\n",
    "\n",
    "#Signed value range of the input \n",
    "input_min_val=-2.0**(input_bits-1.0)\n",
    "input_max_val=2.0**(input_bits-1.0)-1.0\n",
    "\n",
    "print(f\"input value has max range [{input_min_val}, {input_max_val}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# A qnn is a Brevitas version of pytorch's nn. nn stands for neural network.\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    #Quantize to input_bits\n",
    "    bit_width = input_bits\n",
    "    \n",
    "    #Min max value of the input. Set to the range value of the input \n",
    "    min_val = input_min_val #the min value of the input(dataset) before going through the model\n",
    "    max_val = input_max_val #the max value of the input(dataset) before going through the model\n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "model_class = nn.Sequential(\n",
    "    # Input quantization layer\n",
    "    qnn.QuantHardTanh(act_quant=InputQuantizer),\n",
    "\n",
    "    qnn.QuantConv1d(2, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits,bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool1d(2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    qnn.QuantLinear(filters_conv*8, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits, return_quant_tensor=True),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, 27, weight_bit_width=w_bits, bias=True, bias_quant=Int8Bias),\n",
    ")\n",
    "model=model_class\n",
    "\n",
    "import torchinfo\n",
    "print(torchinfo.summary(model_class,input_size=(1,2,1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    # Save losses here and make sure model is in training mode.\n",
    "    losses = []\n",
    "    model.train()    \n",
    "\n",
    "    # Iterate over the data and train\n",
    "    for (inputs, target, snr) in tqdm(train_loader, desc=\"Training Batches\"):#, leave=False):   \n",
    "        #if gpu is not None:\n",
    "        inputs = inputs.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "                \n",
    "        # forward pass\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "    return losses\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for (inputs, target, snr) in tqdm(test_loader, desc=\"Testing Batches\", leave=False):\n",
    "            #if gpu is not None:\n",
    "            inputs = inputs.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            output = model(inputs)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpt_path=Path('27ml_rf/model_brevitas.pth')\n",
    "onnx_file=Path('27ml_rf/model_brevitas.onnx')\n",
    "\n",
    "print(f'Model parameters will be saved in {chpt_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are training the model for 100 epochs with a batch size of 1024 by default.\n",
    "These numbers can be adjusted. We use an early stop of 10 epochs\n",
    "\n",
    "The first epoch usually takes longer to train. \n",
    "After that, it should train faster per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "batch_size = 1024\n",
    "num_epochs = 100\n",
    "early_stop = 10\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size, sampler=dataset.train_sampler)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)\n",
    "data_loader_val = DataLoader(dataset, batch_size=batch_size, sampler=dataset.val_sampler)\n",
    "\n",
    "\n",
    "model = model.to(gpu)\n",
    "\n",
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(gpu)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "training_time=time.time()\n",
    "best_val_acc = float('-inf')\n",
    "count = 0\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        val_acc = test(model, data_loader_val)\n",
    "        print(\"Epoch %d: Training loss = %f, validation accuracy = %f\" % (epoch, np.mean(loss_epoch), val_acc))\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), chpt_path)\n",
    "            print(f'Model checkpoint is saved in {chpt_path}')\n",
    "    \n",
    "            export_qonnx(model.to('cuda'), torch.randn(1, 2, 1024).to('cuda'), export_path=onnx_file)\n",
    "            qonnx_cleanup(str(onnx_file), out_file=str(onnx_file))\n",
    "            print(f'QOnnx Model checkpoint is saved in {onnx_file}')\n",
    "            best_val_acc = val_acc\n",
    "            count = 0\n",
    "        else:\n",
    "            count+=1\n",
    "\n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(val_acc)\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "        if count > early_stop:\n",
    "            print(\"Stopping early\")\n",
    "            break\n",
    "\n",
    "training_time=time.time()-training_time\n",
    "print(f'total training time: {training_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over epochs\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test accuracy over epochs\n",
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained parameters to disk\n",
    "torch.save(model.state_dict(), chpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Load the model back again, or you can change the file to load a different model\n",
    "# load_path=chpt_path #Change this to a path of a different model\n",
    "\n",
    "# model=model_class \n",
    "# model.load_state_dict(torch.load(load_path))\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a fresh test data loader\n",
    "# batch_size = 1024\n",
    "# dataset = radioml_21_dataset(dataset_path)\n",
    "# data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on validation data\n",
    "y_exp = np.empty((0))\n",
    "y_snr = np.empty((0))\n",
    "y_pred = np.empty((0,len(dataset.mod_classes)))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader_test, desc=\"Batches\"):\n",
    "        inputs, target, snr = data\n",
    "        inputs = inputs.to(gpu).float()\n",
    "        output = model(inputs)\n",
    "        y_pred = np.concatenate((y_pred,output.cpu()))\n",
    "        y_exp = np.concatenate((y_exp,target))\n",
    "        y_snr = np.concatenate((y_snr,snr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix across all SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall confusion matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=90)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "for i in range(len(y_exp)):\n",
    "    j = int(y_exp[i])\n",
    "    k = int(np.argmax(y_pred[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(dataset.mod_classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_confusion_matrix(confnorm, labels=dataset.mod_classes)\n",
    "\n",
    "cor = np.sum(np.diag(conf))\n",
    "ncor = np.sum(conf) - cor\n",
    "print(\"Overall Accuracy across all SNRs: %f\"%(cor / (cor+ncor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix at 4 specific SNRs\n",
    "Notice how the accuracy is very low at lower SNR and the accuracy is very high at higher SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices at 4 different SNRs\n",
    "snr_to_plot = [-20,-4,+4,+30]\n",
    "plt.figure(figsize=(16,10))\n",
    "acc = []\n",
    "for snr in dataset.snr_classes:\n",
    "    # extract classes @ SNR\n",
    "    indices_snr = (y_snr == snr).nonzero()\n",
    "    y_exp_i = y_exp[indices_snr]\n",
    "    y_pred_i = y_pred[indices_snr]\n",
    " \n",
    "    conf = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    confnorm = np.zeros([len(dataset.mod_classes),len(dataset.mod_classes)])\n",
    "    for i in range(len(y_exp_i)):\n",
    "        j = int(y_exp_i[i])\n",
    "        k = int(np.argmax(y_pred_i[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(dataset.mod_classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    " \n",
    "    if snr in snr_to_plot:\n",
    "        plot, = np.where(snr_to_plot == snr)[0]\n",
    "        plt.subplot(221+plot)\n",
    "        plot_confusion_matrix(confnorm, labels=dataset.mod_classes, title=\"Confusion Matrix @ %d dB\"%(snr))\n",
    " \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    acc.append(cor/(cor+ncor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy over SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over SNR\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(dataset.snr_classes, acc, marker='o')\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.xlim([-20, 30])\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title(\"Classification Accuracy over SNR\")\n",
    "plt.grid()\n",
    "plt.title(\"Classification Accuracy over SNR\");\n",
    "\n",
    "print(\"Accuracy @ highest SNR (+30 dB): %f\"%(acc[-1]))\n",
    "print(\"Accuracy overall: %f\"%(np.mean(acc)))\n",
    "for i, a in enumerate(acc):\n",
    "    print(f\"SNR [{-20 + 2*i}]: {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of the accuracy of each modulations over SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy per modulation\n",
    "accs = []\n",
    "for mod in range((dataset.num_classes)):\n",
    "    accs.append([])\n",
    "    for snr in dataset.snr_classes:\n",
    "        indices = ((y_exp == mod) & (y_snr == snr)).nonzero()\n",
    "        y_exp_i = y_exp[indices]\n",
    "        y_pred_i = y_pred[indices]\n",
    "        cor = np.count_nonzero(y_exp_i == np.argmax(y_pred_i, axis=1))\n",
    "        accs[mod].append(cor/len(y_exp_i))\n",
    "        \n",
    "# Plot accuracy-over-SNR curve\n",
    "plt.figure(figsize=(12,8))\n",
    "for mod in range(dataset.num_classes):\n",
    "    if accs[mod][-1] < 0.95 or accs[mod][0] > 0.1:\n",
    "        color = None\n",
    "    else:\n",
    "        color = \"black\"\n",
    "    plt.plot(dataset.snr_classes, accs[mod], label=str(mod) + \": \" + dataset.mod_classes[mod], color=color)\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.xlim([-20, 30])\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.title(\"Accuracy breakdown\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model as QONNX (Quantized ONNX) file\n",
    "\n",
    "QONNX is just a extended version of ONNX file. \n",
    "\n",
    "QONNX file is still saved under the extension `.onnx`, but their parameters are quantized\n",
    "\n",
    "To standardize the project, we will call:\n",
    "- QONNX Model exported from Brevitas: `_brevitas.onnx`\n",
    "- QONNX Model after network surgery: `_finn.onnx`\n",
    "\n",
    "Further information about FINN-ONNX and network surgery is explained in the `build_transformed_model.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the model so that FINN can regconize nodes\n",
    "\n",
    "Ensure the model is cleaned up before converting to FINN ONNX, so that the nodes in the model are correctly labeled. Otherwise, when we convert to FINN-ONNX, the converter may not be able to regconize the nodes.\n",
    "\n",
    "Source code for cleanup(): https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/util/cleanup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "model.eval()\n",
    "build_dir=\"27ml_rf/models\" #Directory to save model\n",
    "\n",
    "#Ensuring path exist, otherwise create an empty directory\n",
    "Path(build_dir).mkdir(exist_ok=True)\n",
    "export_path=f\"{build_dir}/radio_27ml_brevitas.onnx\" #Full name of the path of the model with the tail _export.onnx\n",
    "export_qonnx(model.to('cuda'), torch.randn(1, 2, 1024).to('cuda'), export_path=export_path);\n",
    "\n",
    "qonnx_cleanup(export_path, out_file=export_path)\n",
    "print(f'model is saved in {export_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
